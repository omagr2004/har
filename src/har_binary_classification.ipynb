{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Data Subset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/har_binary_classification.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "def load_data():\n",
    "    # Load training data\n",
    "    X_train = pd.read_csv('../data/UCI HAR Dataset/train/X_train.txt', sep='\\\\s+', header=None)\n",
    "    y_train = pd.read_csv('../data/UCI HAR Dataset/train/y_train.txt', sep='\\\\s+', header=None)\n",
    "    \n",
    "    # Load test data\n",
    "    X_test = pd.read_csv('../data/UCI HAR Dataset/test/X_test.txt', sep='\\\\s+', header=None)\n",
    "    y_test = pd.read_csv('../data/UCI HAR Dataset/test/y_test.txt', sep='\\\\s+', header=None)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Filter data for binary classification (Walking vs. Not Walking)\n",
    "def filter_binary_classification(X, y):\n",
    "    y_binary = y[0].map(lambda x: 1 if x == 1 else 0)  # 1 for Walking, 0 for Not Walking\n",
    "    return X, y_binary\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data()\n",
    "X_train, y_train = filter_binary_classification(X_train, y_train)\n",
    "X_test, y_test = filter_binary_classification(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/har_binary_classification.py\n",
    "\n",
    "# Normalize the data\n",
    "scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Segment the data into fixed-size windows\n",
    "def segment_data(X, window_size=128):\n",
    "    segments = []\n",
    "    for start in range(0, len(X) - window_size + 1, window_size):\n",
    "        segment = X[start:start + window_size]\n",
    "        segments.append(segment)\n",
    "    return np.array(segments)\n",
    "\n",
    "X_train_segments = segment_data(X_train_scaled)\n",
    "X_test_segments = segment_data(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Model Building & Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 283ms/step - accuracy: 0.5563 - loss: 0.8029 - val_accuracy: 0.9167 - val_loss: 0.9109\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step - accuracy: 0.8546 - loss: 1.5083 - val_accuracy: 0.9167 - val_loss: 0.2506\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step - accuracy: 0.7553 - loss: 0.4477 - val_accuracy: 1.0000 - val_loss: 0.4039\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 98ms/step - accuracy: 0.5519 - loss: 0.9250 - val_accuracy: 0.9167 - val_loss: 0.2236\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 106ms/step - accuracy: 0.8190 - loss: 0.4672 - val_accuracy: 0.9167 - val_loss: 0.2157\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 109ms/step - accuracy: 0.8546 - loss: 0.3698 - val_accuracy: 0.9167 - val_loss: 0.1641\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step - accuracy: 0.8694 - loss: 0.3172 - val_accuracy: 0.9167 - val_loss: 0.2131\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step - accuracy: 0.9451 - loss: 0.2245 - val_accuracy: 0.9167 - val_loss: 0.1847\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8738 - loss: 0.2546 - val_accuracy: 0.9167 - val_loss: 0.1024\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step - accuracy: 0.8694 - loss: 0.3261 - val_accuracy: 0.9167 - val_loss: 0.1072\n"
     ]
    }
   ],
   "source": [
    "# src/har_binary_classification.py\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "\n",
    "# Build a simple CNN model with Dropout layers\n",
    "def build_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Conv1D(64, kernel_size=3, activation='relu'),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.5),\n",
    "        Flatten(),\n",
    "        Dense(100, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "                  loss='binary_crossentropy', \n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def segment_labels(y, window_size=128):\n",
    "\tsegments = []\n",
    "\tfor start in range(0, len(y) - window_size + 1, window_size):\n",
    "\t\tsegment = y[start:start + window_size]\n",
    "\t\tsegments.append(segment.iloc[0])  # Take the first label in the segment\n",
    "\treturn np.array(segments)\n",
    "\n",
    "y_train_segments = segment_labels(y_train)\n",
    "\n",
    "input_shape = (X_train_segments.shape[1], X_train_segments.shape[2])\n",
    "model = build_model(input_shape)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_segments, y_train_segments, epochs=10, batch_size=32, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
      "Accuracy: 0.8260869565217391\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "Confusion Matrix:\n",
      "[[19  0]\n",
      " [ 4  0]]\n"
     ]
    }
   ],
   "source": [
    "# src/har_binary_classification.py\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# Segment the labels\n",
    "def segment_labels(y, window_size=128):\n",
    "\tsegments = []\n",
    "\tfor start in range(0, len(y) - window_size + 1, window_size):\n",
    "\t\tsegment = y[start:start + window_size]\n",
    "\t\tsegments.append(segment.iloc[0])  # Take the first label in the segment\n",
    "\treturn np.array(segments)\n",
    "\n",
    "y_test_segments = segment_labels(y_test)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test_segments)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test_segments, y_pred_binary)\n",
    "precision = precision_score(y_test_segments, y_pred_binary,zero_division=0)\n",
    "recall = recall_score(y_test_segments, y_pred_binary)\n",
    "conf_matrix = confusion_matrix(y_test_segments, y_pred_binary)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
